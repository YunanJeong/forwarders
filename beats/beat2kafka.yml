###################### Filebeat Configuration Example #########################

# This file is an example configuration file highlighting only the most common
# options. The filebeat.reference.yml file from the same directory contains all the
# supported options with more comments. You can use it as a reference.
#
# You can find the full configuration reference here:
# https://www.elastic.co/guide/en/beats/filebeat/index.html

# For more available modules and options, please see the filebeat.reference.yml sample
# configuration file.

# 서비스 조회:                               services.msc
# MSI 설치시, 설정파일 경로:                  C:\Program Files\Elastic\Beats\9.0.1\filebeat\filebeat.yml
# MSI 설치시, 로그 및 오프셋 레지스트리 경로:  C:\ProgramData\filebeat
# Ubuntu 서비스 조회:                        sudo systemctl status filebeat
# DEB 설치시, 설정파일 경로:                  /etc/filebeat/filebeat.yml
# DEB 설치시, 로그 및 오프셋 레지스트리 경로:  /var/lib/filebeat

# ============================== Filebeat inputs ===============================

filebeat.inputs:

# Each - is an input. Most options can be set at the input level, so
# you can use different inputs for various configurations.
# Below are the input-specific configurations.

# filestream is an input for collecting log messages from files.
- type: filestream
  id: my-filestream-id
  enabled: true
  # 로그수집 대상 파일을 지정
  # glob 패턴의 와일드카드(*)을 사용하여 여러 파일을 지정가능
  # yaml 형식의 리스트로 여러 경로를 지정가능
  # 윈도우 경로 표기시, 슬래시(/) 사용가능
  # 로거에 의해 로테이션된 파일도 대상으로 지정해야 로그누락이 없음 (로거가 rename방식으로 로테이션 처리하는 경우)
  paths: 
    - C:/foo/bar/world*/Log/**/*.log
    #- c:\programdata\elasticsearch\logs\*

  # filestream input에서 단일 메시지 최대 길이 제한
  message_max_bytes: 10485760 # default: 10485760 (10MB)

  # 동시에 읽을 파일(핸들러) 수를 제한 # Default는 0으로, 무제한
  # 값을 설정하여 과부하 및 File Descriptor 이슈를 방지
  harvester_limit: 500

  # 지정 시간만큼의 최근 파일만 모니터링. 시간이 지나면 무시. (파일수정시각으로 판단)
  # Default는 0으로, 로그수집 대상경로 내 모든 파일을 모니터링
  # 값을 설정하여 과부하를 방지 # 서버 점검시간보다는 넉넉하게 길어야 합니다. (24h~168h 권장)
  # close.on_state_change.inactive 보다 길어야 함
  # 주의: filebeat 최초 실행시에도 적용되므로, 기존 파일이 있다면 지정 시간보다 이전의 파일은 수집되지 않음
  ignore_older: 48h

  ### Harvester(file handler) closing options
  # 파일을 "마지막으로 읽은 시점"(파일 mtime 아님) 후, 지정시간 동안 신규로그가 없으면 harvester를 close함
  # close되어도 ignore전까진 파일 mtime 변경(=신규로그발생) 시, 다시 open하여 읽음.
  # "신규 로그 생성 주기"보다 커야 함(하베스터 수 제한 대기로 실시간성 저하 가능).
  # ignore_older보단 작아야 함.
  # 주의: close 순간 신규 로그가 생기면 그 로그를 못 읽을 수 있음.
  #       다음 신규 로그가 또 생기면 다시 open하여 모두 읽지만,
  #       close 당시 로그가 그 파일의 마지막 로그면 누락 위험.
  #       매우 간헐적으로 갱신되는 파일을 읽을시 이 설정을 크게하는게 안전
  # default: 5m
  close.on_state_change.inactive: 5m 

  # If enabled, instead of relying on the device ID and inode values when comparing files,
  # compare hashes of the given byte ranges in files. A file becomes an ingest target
  # when its size grows larger than offset+length (see below). Until then it's ignored.
  # Filebeat 8.x부터 파일 고유성 판단에 fingerprint(앞부분 1024바이트 해시) 사용 (default)
  # - inode 등 메타데이터 미사용, 앞부분이 같으면 같은 파일로 인식
  # - 로그 앞부분에 유니크 값(시간, 서버명 등) 있으면 정확도↑
  # - 파일 로테이션 환경에 유리
  # - 1024바이트 미만 작은 파일은 로그수집 누락되며, fingerprint 비활성화 또는 크기 조정으로 해결 가능
  # - 비활성화 시 기존처럼 inode 등으로 판단(재사용시 오탐 가능)
  # 현재 fingerprint를 끄면 로그수집이 제대로 안되는 문제가있는데, 이건 뭔지 모르겠다. 확인필요. Registry 동작도 다르게하는듯??(추정)
  # prospector.scanner.fingerprint.enabled: true

  # If fingerprint mode is enabled, sets the offset from the beginning of the file
  # for the byte range used for computing the fingerprint value.
  # 이미 실행중인 filbeat에서 변경하면 안됨. 최초시작시만 설정
  #prospector.scanner.fingerprint.offset: 0

  # Fingerprint 기능: 파일의 고유성을 판단하기 위해, 파일 앞부분 일정길이를 활용합니다.
  # 이 길이가 너무 작으면, 파일의 고유성을 구분하지 못해 로그수집 누락이 발생할 수 있습니다.
  # 너무 크면, 해당 용량 바이트수보다 작은 파일은 로그수집 실패하므로 주의가 필요합니다.
  # - 파일 앞부분이 같으면 같은 파일로 인식
  # - 로그 앞부분에 유니크 값(시간, 서버명 등) 있으면 고유성↑
  # - 파일 로테이션 환경에 유리
  # If fingerprint mode is enabled, sets the length of the byte range used for
  # computing the fingerprint value. Cannot be less than 64 bytes.
  # default: 1024
  prospector.scanner.fingerprint.length: 256   

  # Exclude lines. A list of regular expressions to match. It drops the lines that are
  # matching any regular expression from the list.
  # Line filtering happens after the parsers pipeline. If you would like to filter lines
  # before parsers, use include_message parser.
  #exclude_lines: ['^DBG']

  # Include lines. A list of regular expressions to match. It exports the lines that are
  # matching any regular expression from the list.
  # Line filtering happens after the parsers pipeline. If you would like to filter lines
  # before parsers, use include_message parser.
  #include_lines: ['^ERR', '^WARN']

  # Exclude files. A list of regular expressions to match. Filebeat drops the files that
  # are matching any regular expression from the list. By default, no files are dropped.
  #prospector.scanner.exclude_files: ['.gz$']

  # Optional additional fields. These fields can be freely picked
  # to add additional information to the crawled log files for filtering
  #fields:
  #  level: debug
  #  review: 1

# journald is an input for collecting logs from Journald
#- type: journald

  # Unique ID among all inputs, if the ID changes, all entries
  # will be re-ingested
  #id: my-journald-id

  # The position to start reading from the journal, valid options are:
  #  - head: Starts reading at the beginning of the journal.
  #  - tail: Starts reading at the end of the journal.
  #    This means that no events will be sent until a new message is written.
  #  - since: Use also the `since` option to determine when to start reading from.
  #seek: head

  # A time offset from the current time to start reading from.
  # To use since, seek option must be set to since.
  #since: -24h

  # Collect events from the service and messages about the service,
  # including coredumps.
  #units:
    #- docker.service



# ============================== Filebeat modules ==============================

filebeat.config.modules:
  # Glob pattern for configuration loading
  path: ${path.config}/modules.d/*.yml

  # Set to true to enable config reloading
  reload.enabled: false

  # Period on which files under path should be checked for changes
  #reload.period: 10s


# ================================== General ===================================

# The name of the shipper that publishes the network data. It can be used to group
# all the transactions sent by a single shipper in the web interface.
#name:

# The tags of the shipper are included in their field with each
# transaction published.
#tags: ["service-X", "web-tier"]

# Optional fields that you can specify to add additional information to the
# output.
#fields:
#  env: staging


# ================================== Outputs ===================================

# Configure what output to use when sending the data collected by the beat.

# -------------------------------- Kafka Output --------------------------------
output.kafka:
  # Boolean flag to enable or disable the output module.
  enabled: true

  # The list of Kafka broker addresses from which to fetch the cluster metadata.
  # The cluster metadata contain the actual Kafka brokers events are published
  # to.
  hosts: ["wsl:9095"]

  # The Kafka topic used for produced events. The setting can be a format string
  # using any event field. To set the topic from document type use `%{[type]}`.
  topic: beats2kfk-direct1



  # Metadata update configuration. Metadata contains leader information
  # used to decide which broker to use when publishing.
  #metadata:
    # Max metadata request retry attempts when cluster is in middle of leader
    # election. Defaults to 3 retries.
    #retry.max: 3

    # Wait time between retries during leader elections. Default is 250ms.
    #retry.backoff: 250ms

    # Refresh metadata interval. Defaults to every 10 minutes.
    #refresh_frequency: 10m

    # Strategy for fetching the topics metadata from the broker. Default is false.
    #full: false

  # The number of times to retry publishing an event after a publishing failure.
  # After the specified number of retries, events are typically dropped.
  # Some Beats, such as Filebeat, ignore the max_retries setting and retry until
  # all events are published.  Set max_retries to a value less than 0 to retry
  # until all events are published. The default is 3.
  # Kafka Client Library의 설정으로서 max_retries가 있지만, filebeat는 자체적으로 이 값을 무시하고 계속 재시도한다. (https://www.elastic.co/docs/reference/beats/filebeat/kafka-output)
  #max_retries: 3

  # The number of seconds to wait before trying to republish to Kafka
  # after a network error. After waiting backoff.init seconds, the Beat
  # tries to republish. If the attempt fails, the backoff timer is increased
  # exponentially up to backoff.max. After a successful publish, the backoff
  # timer is reset. The default is 1s.
  #backoff.init: 1s

  # The maximum number of seconds to wait before attempting to republish to
  # Kafka after a network error. The default is 60s.
  #backoff.max: 60s

  # The maximum number of events to bulk in a single Kafka request. The default
  # is 2048.
  #bulk_max_size: 2048

  # Duration to wait before sending bulk Kafka request. 0 is no delay. The default
  # is 0.
  #bulk_flush_frequency: 0s

  # The number of seconds to wait for responses from the Kafka brokers before
  # timing out. The default is 30s.
  #timeout: 30s

  # The maximum duration a broker will wait for number of required ACKs. The
  # default is 10s.
  #broker_timeout: 10s

  # The number of messages buffered for each Kafka broker. The default is 256.
  #channel_buffer_size: 256

  # The keep-alive period for an active network connection. If 0s, keep-alives
  # are disabled. The default is 0 seconds.
  #keep_alive: 0

  # Sets the output compression codec. Must be one of none, snappy and gzip. The
  # default is gzip.
    # 압축전송(gzip: CPU 부하가 미세하게 증가하나, 네트워크 대역폭 면에서 압도적으로 유리함)
  compression: gzip  # snappy: 빠른처리, 낮은CPU부하   # gzip: 높은 압축률, CPU부하증가, 네트워크 비용 감소

  # Set the compression level. Currently only gzip provides a compression level
  # between 0 and 9. The default value is chosen by the compression algorithm.
  #compression_level: 4

  # The maximum permitted size of JSON-encoded messages. Bigger messages will be
  # dropped. The default value is 1000000 (bytes). This value should be equal to
  # or less than the broker's message.max.bytes.
  # Kafka Client Library의 단일 메시지 전송 크기 제한
  # Kafka 측 설정값과 같거나 작아야 함
  # filebeat input은 10MB까지 읽지만, OUTPUT KAFKA의 default는 1000000(약 1MB)
  max_message_bytes: 1000000

  # The ACK reliability level required from broker. 0=no response, 1=wait for
  # local commit, -1=wait for all replicas to commit. The default is 1.  Note:
  # If set to 0, no ACKs are returned by Kafka. Messages might be lost silently
  # on error.
  # default 1 (acks=1)
  # 고가용성 보장시 -1 (acks=all)
  required_acks: -1

  # The configurable ClientID used for logging, debugging, and auditing
  # purposes.  The default is "beats".
  #client_id: beats


# ------------------------------ Logstash(or FLUENTD) Output -------------------
# output.logstash:
#   # The Logstash hosts
#   hosts: ["wsl:5044"]


# ================================= Processors =================================
processors:
  - drop_fields:
      # filebeat가 기본포함시키는 메타데이터 필드 일부 제거
      fields: ["ecs", "input", "agent", "log.file.idxhi", "log.file.idxlo", "log.file.vol"] 
  # - add_host_metadata: 
      # when.not.contains.tags: forwarded
  # - add_cloud_metadata: ~
  # - add_docker_metadata: ~
  # - add_kubernetes_metadata: ~


# ================================== Logging ===================================

# Sets log level. The default log level is info.
# Available log levels are: error, warning, info, debug
#logging.level: debug

# At debug level, you can selectively enable logging only for some components.
# To enable all selectors, use ["*"]. Examples of other selectors are "beat",
# "publisher", "service".
#logging.selectors: ["*"]
